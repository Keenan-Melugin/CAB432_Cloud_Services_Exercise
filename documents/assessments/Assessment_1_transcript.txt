Okay. Hello. In this video, I'm going to be talking about the first assessment task of CAB 432, which is about building a rest API. Here I've got to open the Canvas page describing the assessment task. You can find it by going to modules and then in the about assessment section. Given that you're watching this video, you probably already know how to find this page. Let's take a look through it together. Beginning with the overview. The task description reads, you'll implement a rest application programming interface to some application that you've deployed to the Cloud using a software container. Take note that this assessment item is eligible for the 48 hour extension, which you can claim for free. If you need an extension beyond that, you do need to apply through the usual mechanisms. Here we're assessing unit learning outcomes one, two, and three, and the assessment task is weighed 30% of your final grade. It's an individual effort, and we're going to be grading you out of 30 marks. We've got some resources here at the top of the assignment description. Just high level links to AWS documentation, Docker documentation. There's a list of APIs in the appendix of this document that you might be interested in working with. But otherwise, the documentation for specific areas of the technologies we've been working with are available on the PAC pages and spread throughout lectures as well. Okay. What do you need to do? If you haven't already, read the introduction to the assessment structure of the unit in the page called assessment overview. I'm going to let you do that in your own time. But otherwise, this assessment task is about laying groundwork for the broader Cloud project that you'll be working on throughout the semester. Here you'll be designing and implementing the application logic, and later, you'll be expanding that by taking advantage of more of the Cloud services. To grade your application, you'll be having to submit source code, a five minute video demonstration of your app functioning, and a short document that summarises how you've responded to the marking criteria. Some notes while this assessment item is strictly individual, later assessments will allow you to work with partner, in which case you can choose one of your two applications to continue with, or you can start a brand new one. Maybe think about that now. However, we are expecting you to work individually for this one. Below, we give a summary of the criteria, the complete assessment criteria will be on the Canvas submission page. Application should satisfy the conditions in a non trivial, non trivial way, creating a cohesive application, features that added purely to satisfy the criteria without any without meaningfully adding functionality to the app simply won't be accepted. Make sure that the app that you're building actually makes sense. You want to demonstrate to us that you've learnt the intended learning outcomes of the unit, but also that the way that you're using these things actually makes sense in the context of the app that you're building. Around this assessment task, there are a set of core criteria and some additional criteria. You must attempt the core criteria. These are the ones that will get you a passing mark. The additional criteria gain you bonus marks heading towards the higher ranked grades. These core tasks also support your later assessments. It's really critical that you get these ones down because assessment items two and three are very much going to build upon these. So the core criteria are worth 20 out of 30 marks. There are six of them that are weighted three marks each, and there's one additional one that's weighted at two marks, and that's 18 plus two getting us to 20 marks. Your application needs to load down the CPU. For three marks, we're looking for you to build some app that has a CPU intensive process. In later assessments, you'll need to load down more than just one CPU. You need to spin up multiple servers. We'll talk about how to do that later in the semester, and you'll be spinning up and reducing the number of servers that you have in the cluster based on the workload. In this assessment task, it's very much focusing on um having one so running and loading it down by around 80% for an extended period of time. We're looking for about 5 minutes. It needs to be quite intensive task. One of the key examples that we provide is video transcoding, transforming a video in its original format to a different format that can take a particularly long time if the video is of really high resolution or high frame rate or is just a really long video. The next core task is looking at how do you actually demonstrate that your server is being put under load in excess of 80% of a CPU for any send time. What is your method around that? Because this test, this load testing method is not specifically about the application itself. It's more adjacent to demonstrating your assessment. This one's worth two marks. It can just be a simple script that sends some requests to the API, which then triggers the CPU intensive task. The next core criteria is around data types. You need to be storing at least two different types of data, which does not include user log in data. We're mostly thinking structured versus unstructured data. Unstructured data is something like the video that you're going to be transcoding. Structured data is more like information about that video. The metadata, things that you would put in a database. You're not going to put the video itself in a database because that belongs in a file system. What does go on your database is where to find that video file in the file system. Think about what's the difference between the raw data that I need for the core um process in my application versus the data that I need to make this application function for the end user. The next core criteria is about containerizing the app. Bundling up your app into a docker container, and then storing that container in our ECR, the Elastic Container registry in the AWS account. The next criteria very tightly is embedded within this one as well. The next step would be to pull down your container out of ECR and run it on an EC two instance. You need to build a rest API, and application programming interface for your application so that people can actually interact with it. So you're going to need a rest API that has endpoints like get jobs, create job, update job, delete job, for instance, and a job could be that CPU intensive process. Create job could be the endpoint that triggers the video transcoding, for example, The last one, user log in, also three marks. This is about providing some basic user login to your API. Once the user has successfully logged in, the server will return to them a JWT, which is then used for subsequent requests through the API. U Logins should have meaningful distinctions for different users. It's important to note in this assessment task, you don't need to have a proper authentication system where you can create users and delete users and have them update their passwords and have a database driven authentication method. The users can be hard coded in this first assessment task. In later assessment tasks, we'll look at managed services by AWS that handle authentication for you in a much more robust way than what we'd be able to implement. The additional criteria, these ones have lower priority than the core criteria, because they're just the bonus marks that you can gain for doing a bit more work. The core criteria, as I said, are very much focused around forward thinking, heading into assessments two and three. Here we've provided five different options for you, but you can only attempt up to four of them. That's because each additional criteria is worth 2.5 marks, and you can only gain up to ten in this section. If you do attempt five, we're just not going to mark the fifth one because otherwise you're going to be gaining 12.5 marks, which is exceeding the available ten marks. It's really important that you tell us specifically which additional criteria you have attempted. You will do that in a document that we provide you. We refer to it as the response to marking criteria document, and you'll show us you'll point us towards the evidence in your source code and your demonstration video and just a brief statement about how you addressed these criteria. The first one is about extending the features of your API. Here we're looking for a really robust API that could be used by any client application. The purpose of building a rest API is that you're providing a generic user interface to your server, that exchanges messages using a structured data format, usually JSON. And by providing that interface, any client app can take advantage of the service that you're offering. Those client apps could be a web application that runs in a standard web browser. It could be a mobile app, I could even be a command line interface or some automation workflow. This criteria is about building a robust and feature rich API. You could consider features of your API like versioning so that when the client apps are using your API, they can specify which version they are compatible with so that your server is able to respond to their requests appropriately. That way you can also upgrade your API and not break client apps by doing so. It could be that you implement pagination. If a request comes in to get a whole list of items or a whole list of records out of a database from your service, rather than responding with 1 million records, maybe you respond with 100 at a time and you explicitly tell that client app, this is how you find the next hundred. This is page two of the result set. Or it could be that you're implementing filtering, which reduces the number of results returned to the requester. You could have filters like the title of the object needs to match this particular string, basically doing a search, or maybe only return objects that were created within a date range or something like that, or it could be that you added the feature of s, the results that are generated by your API are sorted in some particular way. The next additional criteria is about external APIs. You use of third party APIs that bring data into your application to add a novel feature. You can't just implement what would effectively be a proxy. What this means is that if you're using a third party API, you need to be using the data that the third party API provides to you in a really meaningful way. It can't just be that your server requests data from a different server and then passes it to your client and does nothing with it. It simply just acts as a proxy. That has no meaning. You need to actually use that data in some way that provides benefit to your end user. The next one is around additional data types. This strongly reflects back on to the data type criteria in the core section, but this one is about taking it even further. Thinking about different types of storage, different formats, different manipulation techniques. The next one, custom processing. This app is all about having some process that puts your CPU under load. You could use a pre built process that can do that. If we think about the video transcoding example, FFM peg is an excellent tool for doing video transcoding and many other things as well. However, a pre tool, it's available to you under an open source licence. You can just install it on your server and give it commands and it will start manipulating videos in some way and therefore put the CPU under load. Additional criteria is all about implementing some bespoke application where you haven't leveraged a pre built tool. If you've put a lot of effort into some process that is CV intensive, and we want to give you the opportunity to gain marks for doing that. You can take advantage of this additional criteria if you'd like to. The next one is about using infrastructure as code. Yes, you could be spinning up these like the EC two instance and your docker containers using different consoles like the AWS web console or like a console over an SSH connection and be manually specifying the commands of how to launch these things. That's fine, but it's not reproducible. If you needed to duplicate that environment, you need to go and manually do that, rerun all those commands, and it becomes quite tedious. Whereas with infrastructure's code, you're describing the infrastructure needed to run your service using code. Therefore, all you have to do is hit run on that code, and next thing you've got that whole environment created once again with minimal effort. This additional criteria is all about using infrastructure as code to achieve that. It could be around using Docker compose for multi container setups. It could be using clad formation for spinning up your AOBS services, using the CK or maybe something else if you've had a chat to us about what that technology is. The last one is very broad. It is labelled as up on request because maybe there's something that you're really interested in doing in this project. That doesn't quite fit in the other criteria that we've provided, and you still want to gain some marks for it. U you need to speak to the coordinators about this first because you should gain approval and we should have a discussion about what your plan is around this. We don't want you going down the wrong path and doing something wildly different to what we want you to be doing and effectively that being a waste of time. If you want to do the upon request criteria, make sure you actually request it with us prior to the due date so we can have a proper conversation about it. Here we've got some anti criteria. These ones are describing things that you shouldn't do now because we're going to ask you to do them later. If you do them now, we won't give you marks for it. And you might just be putting yourself under unnecessary load by doing them so soon, or it could be that you're just not doing them in the right way that we'd prefer you to do. Things like user management and robust log in methods, we'll implement that later on. Right now, simply a hard coded list of users and a really basic login process is sufficient. Cloud services, other than using a single virtual machine or ECR, don't go beyond an EC two instance basically because we're going to explore in later assessment tasks, how to spin up more servers, more EC two machines and have them work in a scaling group. Databases. Later on, you'll use managed services for databases. You may want to hold off or make use of a database that is already supported by AWS, something like MCL or some of the others and Later we'll explore those managed services. Another anti criteria is multiple processes. Keep things in a single run time. Because later on, we're going to think very carefully about how do we break down your application into microservices, more services that all work together and communicate using one of the communication patterns that we'll discuss later in the semester. They are the criteria. Let's talk about the application itself. This section describe gives you some guidance on what you need to build and details for some of the core criteria. Be mindful that you're going to be spending a fair amount of time on this project. You probably want to think about how can I take advantage of this project later on, putting it on your regime or on your Github profile or something like that. Do something that you're really interested in and try to do a good job of it because it will be a good way to show potential employees that I've worked with Cloud platforms, which comes with the skills of being able to build applications, software development efforts. There's also software engineering efforts involved because now you're working across software systems to build the broader application. How to choose an application to build. Here's just some suggestions, and you can absolutely ignore them if you want. We'll propose some key questions that you should answer for yourself to help you design your application. Think about what application domain or technology are you interested in? Do you have an interest in machine learning or video production or physics, for example? What will users want to do with your app and what problems will your app solve? Think about the use cases and the user stories. What CPU intensive processes will you use to support those use cases or solve those problems? What data is required to support those processes and use cases, and where will be the source of the data? Here's An example. The application domain is around videos in this case. The use cases, watch your video in high quality, watch your video, overload bandwidth network connection, or upload the video to share with others, or see videos I've uploaded. It's very much a video platform, right? The CPU intensive process here is transcoding videos to a common format in multiple resolutions and qualities. The data includes the videos themselves, the meta data about the video, and the lists of user videos. And the source of the data is the end user providing the video itself. All right. Next section, CPU intensive process and load testing. A core part of the last assessment task will be auto scaling, where you add more servers to your cluster when the application is underload so that you can keep up with demand. And then you can scale back down when your application is no longer underload. You should be able to push your application to the maximum amount of load that you can generate. You might have to think about automating that process because if you're manually doing that, you may not be able to interact with your application fast enough to put it under significant stress. There could be various reasons that your application starts boiling necking. It could be that you CPU is on the load, maybe you're running out of memory. Maybe it's a very io intensive process with the discs, hitting the boundaries of on that. It could be that you have limited network bandwidth or a whole range of other things. It's pretty hard to put AWS under load in terms of its network bandwidth and it's probably best that you don't try to do that. It's very acceptable to put CPUs under load though. Here are some more examples of what you could do. Again, video transcoding, or maybe it could be video generation, could be audio transcoding, perhaps even manipulation or generation or classification of images. It could be that you're working with large language models. It could be that you're training or using machine learning models, some data analysis, pipeline, physics simulations, compiling a large code base and running unit test suites, code static analysis, could be that you're running game server, could be text processing, like sentiment analysis, or a whole lot of other things that you could come up with. Be mindful that when we're looking at what is your CPU intensive process, it shouldn't just be some sort of busy work operation like calculating a really large factorial. What sort of meaning does that have? You need to do something that provides benefit to your end user. Make this a genuine application that someone would actually use in the real world. Before you dive deep into developing your app, you should think about planning and testing your CPU intensive process. To do that, you could create a really bare bones app that has the CPU intensive process embedded and send a request to H HTTP or aRAPI that triggers that CPU intensive process. Maybe at this point, you're not fully fleshing out your API. Deploy this app onto an EC two instance, maybe a T three instance, and then send it some requests, and just check that what you're implementing is actually putting an EC two instance under load. You really do want to try to hit around that 80 or 90% CPU utilisation for about 5 minutes. Also, be mindful that with the T instant types, they come with burst credits, which allows the virtual machine to exceed its CPU baseline performance when it's under load. Not all EC two instant types have this feature. We do have a list of instant types that are available to you. They're on the page called AWS services available on Canvas. Otherwise, you can't explore the full list of instant types that AWS offers by just following that link through. If you're not really sure about what instance type you should use, have a chat with the teaching team because there are a lot of options, and we can help you narrow it down. All right. Load testing. The purpose of load testing is to show you can generate enough load on your app. You can do that using something like postman or another tool that can generate HTTP requests. It could be really bare bones and you're using tools like WG or curl at the command line. This task, as you would have noted in the core criteria, it's weighted slightly less than everything else at two marks because it's not integral to your app. It's really just about enabling you to demonstrate your app. So it's fine if you ask a generative AI to write this script for you. It's a good starting point. Productivity tool. Go for it. Okay. Be mindful of network speed versus CPU speed. S if you're having to upload a really large file to your application, which then puts the application under load by having to process that file. Maybe you're not able to bring the CPU under significant load because it's taking a long time to get that file to the server. It could be because your home Internet connection doesn't have the bandwidth needed to get that file to your server quickly. So consider coming to the campus and uploading where you've got really good throughput on the network. Or it could be that you upload files in advance and then trigger through a different request, the processing of that file. Have a read through the section, and if you're having troubles with putting your CP underload and consider maybe it's the network bandwidth that is causing you issues. More information here about the data requirements. So as I mentioned before, you need to have two types of data, unstructured and structured. Unstructured data is usually things like images, videos, source code, some data sets, like, particularly if it's document based data. Where the shape, the format of this data is basically unpredictable. You really don't know what's in the image. You don't really know what's in the video. You don't really know what's in the source code, whereas with structured data, think of a database table where it's rows and columns. You know the column labels, what their purposes are, and you can probably have a pretty good guess at what's in the records of those tables, or at least there's clear direction about what's in them and you can work in a much more structured way with that data. So do make sure that you're considering both of these data types. And I think that's going to help a lot with this EPU intensive process, because working with unstructured data is usually much more expensive in terms of computational power than structured data. Information about the users. Think about that in the future assessment items, you're going to be implementing user management through a Cloud service. Again, keep your authentication really basic in this assessment task. And take note that user identity information, like user names, e mail addresses, passwords, don't count towards the two different data types of the core criteria in assignment one. You app does need to have some meaningful distinction between data that is specific to particular users. It could be that you have different user roles or permissions that allow users to do more powerful actions in your app compared to others, it could be that you have an admin role in a regular user role where Admins can access all the data in your application where regular users can only access their own data. And you do that by having metadata that describes who owns the data in your system. It could be that your user related functionality is to do with settings about how that user regularly interacts with your application, or it could be application specific functionality like them having bookmarks or playlists or being able to like and comment on things. Here, again, we've mentioned that hardcode credentials are accepted. You probably want at least two separate users to demonstrate that. They log in using the user name and password that had been hard coded, and then the remaining logic beyond that log in process checks a JWT that authenticates them onwards. If you're using an external API, you don't have to use one because this is an additional criteria. But be mindful that you probably don't want to be using one that comes at a cost, whether it's to you or to the university. Do try to find one that is freely avowable, and remain so for the full duration of the semester and maybe even a little bit beyond the semester. It could be that you get like a free trial. Hopefully, you might get a one year free trial, but you're also not limited by the number of requests that you can make before you hit a paywall. Some APIs may allow you to make 100 requests per minute before you have to start paying, or maybe you just get a flat 1,000 requests per month or something. Just be mindful of these sorts of limits that some paid services use to get you in with a free tier. Make sure that if you are going to use a third party API, you get access to it as early as possible. Sometimes, you have to go through a request process where there might be some like human intervention at the other end where they have to approve your request. Be mindful, don't commit your API keys to get. That's a security risk, and you also shouldn't hard code your API keys into your source code. They really should be provided through some flexible mechanism like using environment variables because then you have a single file like an environment's file that describes the sensitive parts of your application. And you don't commit that file to get. You might provide a sample of that file that you commit to get, but you don't commit the actual thing. I've already spoken about limits and restrictions. We recommend that you're not scraping a third party service and rather you're using a proper API. If you are interested in attempting that additional criteria, but you're not quite sure about what API is to use. There's plenty out there. We found these repositories that list public APIs. Take a look at these. Maybe that will help you narrow down something they're interested in doing. Let's talk about the technology stack. Starting with the development environment. You very much need to demonstrate the technologies that we've taught you in the unit. There's loads of possibilities. As I'm sure you've figured out by now, the landscape of technologies available in software development and Cloud computing is vast. Stick with the ones that we've demonstrated to you because then you're explicitly showing us that you're achieving what we want you to achieve. However, it also means that tutors can support you, because if you're going to be using a language or a framework or some other technology that we don't know, we can't support you, and it really is impossible for us to loan all of these given how many there are of you in the cohort. You can develop your app on your local machine. It could be that you're using a Linux box or maybe windows subsystem for Linux or maybe a virtual machine running in tool like virtual box. Or you can use a Cloud instance. But just be mindful, the final deployment of your app must be on AWS. It must be in our Cloud account. If you're using Apple silicon like the M M chip CPU, have a chat with us. Let us know, we'll try to figure out what that means, and if you're going to run into any issues along the way. Regarding deployment, you're required to deploy the app via Docker on top of a publicly available Linux virtual machine. Here we've chosen that you must use EUB Boon 2244 as the base image. You should follow a similar strategy to what we demonstrated in the PAC regarding Docker, like creating a Docker file, then building the image and deploying that as a container. Be mindful of the port mapping, particularly around the security groups that are being implemented in EC two and make sure that if you're exposing ports to make your application function, then they're accessible in the right ways if needed. You should push your docket image to the ECR repository in the CA 4328s account, and then that way you can pull it back down and run it on EC two. The intention should be that your workflow to deploy your app and make it function is very simple, and that very much fits within the nature of using Docker. If you're using a database, then you should deploy that using one of the already available docker images of that database. They certainly exist, and it makes it very simple for you to launch a database just by using a Docker command. If you need to handle that, generally, the documentation of these database engines now describe how to do that in Docker or ask one of the teaching team, we can point you in the right direction. The next part, recommendations for your application structure. While this is not directly assessed, it might make things easier for you later on. Think about different parts of your application logic and how to implement them in a modular fashion. Think about separation of source code into different files. Think about having separate files for different routers, different controllers. Think about how your application is interacting with data, whether it's structured or unstructured, because later on, you're probably going to have to change some of these things to suit the cloud services you'll use in assessments two and three. In regards to special requests, we're happy for you to submit a special request to us, but it needs to be reasonable, well justified, and planned out very carefully, because it's very difficult for us to support different technologies that we haven't already covered in as I said earlier. So you don't need a special request to use the technologies listed here like EC two, DCA, BCR, the CDK, cloud formation, and a few other things. You also don't need a special request to use basically all the web technologies that surround node and Java Script and similar comment for Python. Regarding domain specific technologies. It's generally fine that you use those technologies without request because it makes sense in your context. Just be mindful that again, the teaching staff may not have used that technology you're interested in. So you're very much going to have to read the documentation and learn it for yourself. If you're doing a small amount of programming in a language that's not Python or Java Script. Let's have a chat about it first and understand why that's the case, and maybe we can avoid it. Otherwise, if it makes sense to proceed, then it's okay. If you're doing a substantial amount of programming in a programming language that suits your domain, then definitely have a chat with us about that. Be mindful that we can't approve special requests that defeat the purposes of the core criteria or short circuit some of the work around building the application. We'll typically deny requests to use a technology. If the only reason is to avoid learning the pre approved tools, we very much need you to demonstrate that you've learned something in what we're t. That makes it really easy for us to give you marks, and then you get the highest grade possible. We are going to set the bar high with these requests, so make sure that you present a really strong case around it. Okay. The submission process. Evaluating your apps is quite time consuming and we'll do our best to get your mark as soon as we can. And to make sure that happens quickly, we're asking you to submit three different items, and you very carefully go through that submission process and get it correct so that we don't run into too many speed bumps along the way. So the first piece is your source code. The source code is the ground truth for your project. We'll ask you to upload it separate to the other two items, and we'll review it as needed. Be sure that you're not including source code of installed packages. If you're using node, you'd be familiar with node modules. We don't need a copy of that because you've obtained that from repository that is available on the Internet, so we can do the same. Don't upload node modules because it's enormous. It takes up a lot of space, and it's unnecessary for us to have something that we can get anyway. We're much more interested in your source code and not third party source code. Okay. The response to marking criteria document outlines the features of your app that you want us to mark. Somewhere there, I think it's on the submission process page. You can download the template of this document. And provide your response to the criteria that you addressed. So we're expecting that you address all the core criteria and up to four additional criteria. In that document, you tell us where to find the evidence of you responding to that in the way of the source code and a timestamp in the demonstration video and just provide a brief statement about how you address this, like just a short description. It's probably like a sentence or a couple of lines if really needed. Okay. The demo video itself. This is so that you can show us your project actually functions. Make sure you actually submit the video. We very often see people upload the video, but then they forget the press the submit button. In Canvas, there's two steps, you first upload and then you submit. If you don't submit, we don't actually receive it. From our ends, we can't see that you've uploaded it, so be sure that you're pressing submit. Okay, and just to finish off the page. We'll try to get back to you within ten to 15 working days of submitting. The reason for why we've marked is usually self evident based on the criteria. Otherwise, the marker will provide some written feedback to you as well. We'll go through a moderation process where the whole teaching team first understands what the benchmarks are for the assessment. Then just before releasing marks, we'll check again that each marker across the markers, it's pretty consistent and that we're all on the same page. More information about the submission process can be found on this other page. That's 1.2. The video 5 minutes maximum. Be sure to show us your CPU underload. You need to be underload for about 5 minutes, which is the length of the video, but to fit it within that five minute video, just cut the middle part out, either by trimming the video after you've recorded it or just pause the video halfway through the CPU being underload. Make sure you're capturing your screen at a high enough quality that we can actually read the text in your recording. Um, so we'll do our best to make sure that we're watching your recording at full screen, and we'll definitely play it back at the highest quality available. But sometimes it's useful to us that you zoom in, you make your text font size larger. Generally, you can do that using Control plus, like VS code and your web browser, will increase the font size and just make it a bit easier on the eyes. Make sure you're capturing your screen digitally and not using a physical camera like using your phone camera to record is not acceptable. The purpose of your video is to demonstrate functionality, not explain it. Leave that to the response to criteria document. To record your video, you could record a bunch of short videos and edit them together. You could use a tool like OBS Studio to capture your screen. Canvas also has a built in screen recorder. Once you go to the submission page and you choose to upload a video, there's also the record media option there as well. Otherwise, like I am right now, just Zoom. Open the Zoom meeting by itself, share your screen, and hit record. We're asking you to demonstrate all of these items. I'm not going to run through them because you'll need to revisit this when you record the video for yourself, but it's really like the docker image, EC two, the API action, the types of data, and your CPU process. We don't mind too much about how polished your video is. There's no mark specifically for the video. It's very casual. It's really about showing us what you've achieved, but it's not the core evidence needed to gain marks. Okay, the response to marking criteria document, here is the template for it, and we've also provided a sample response to that template. Um be sure that you follow the template very carefully and you don't modify the template, to make sure that we can maximise the number of marks we award you. If you very clearly describe in this document precisely what you did, and you describe it in a way that we're expecting, the cognitive burden of interpreting this document will be much less and therefore make marking of your attempt easier. That way, we can give you as much marks as we can without too much thinking. Make it really easy for us. Make it really obvious. Okay, the source code. We've provided a bit of a idea around what the source code should look like. Obviously, this one is for a node based app. It's going to vary a bit. You don't need to follow this to the T, but this is the main one. Don't forget to submit your response to criteria Marko document in your source code. Don't include source code of third party packages, as I said, Don't include your Git directory if you're using Git for version control. If you're on a MAC, really don't need the hidden MAC SX directory. It just bloats your submission. If your submission is exceeding 100 megabytes. There's probably something in there that we don't need. Consider deleting things. We certainly don't need data from your app as well, whether it's structured or unstructured, we just want your source code. If it does exceed 100 megabytes, it's actually going to make it quite difficult for us to download your source code. The platforms that we're using for the submission process. For some reason, don't handle zip archives in excess of 100 megabytes very well. If you're exceeding that, just be careful that it could make it really difficult for us to actually mark your assessment. Have a chat with us if that's the case, and we'll figure something out. Yeah, this assessment item is individual, it needs to be your own work. It's okay to copy short snippets of code from the unit content and from the web. But make sure you reference it. We're okay for you to be using chat GPT and other generative AI. Again, leave a comment indicating where you have and be aware that we're using plagiarism detection tools as well. So I think that's all of detail I can cover in getting started video. If you have questions, of course, ask the teaching team. We're here to help you out. We want you to achieve the best outcome possible. There's a lot of detail in this project description. So it might take if you read through to get to get a full understanding of what's required. But there's the team's work space, there's the Q&As, there's the practicals. We're here to help you, take advantage of the time that you have with us. And I'm looking forward to seeing what you've built. 